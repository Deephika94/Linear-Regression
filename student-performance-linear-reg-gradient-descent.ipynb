{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":6051848,"sourceType":"datasetVersion","datasetId":3462333}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-06-21T18:56:44.429635Z","iopub.execute_input":"2024-06-21T18:56:44.430991Z","iopub.status.idle":"2024-06-21T18:56:45.635990Z","shell.execute_reply.started":"2024-06-21T18:56:44.430932Z","shell.execute_reply":"2024-06-21T18:56:45.634624Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/student-performance-multiple-linear-regression/Student_Performance.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"from sklearn.metrics import mean_squared_error\nimport matplotlib.pyplot as plt\nimport time","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:57:00.141098Z","iopub.execute_input":"2024-06-21T18:57:00.141481Z","iopub.status.idle":"2024-06-21T18:57:00.745031Z","shell.execute_reply.started":"2024-06-21T18:57:00.141448Z","shell.execute_reply":"2024-06-21T18:57:00.743969Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data = pd.read_csv(\"/kaggle/input/student-performance-multiple-linear-regression/Student_Performance.csv\",sep=\",\",)\nprint(\"Data dimension: \",data.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:57:00.940307Z","iopub.execute_input":"2024-06-21T18:57:00.940694Z","iopub.status.idle":"2024-06-21T18:57:00.974152Z","shell.execute_reply.started":"2024-06-21T18:57:00.940666Z","shell.execute_reply":"2024-06-21T18:57:00.973093Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Data dimension:  (10000, 6)\n","output_type":"stream"}]},{"cell_type":"code","source":"data.columns","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:57:02.093418Z","iopub.execute_input":"2024-06-21T18:57:02.093853Z","iopub.status.idle":"2024-06-21T18:57:02.103900Z","shell.execute_reply.started":"2024-06-21T18:57:02.093818Z","shell.execute_reply":"2024-06-21T18:57:02.102649Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"Index(['Hours Studied', 'Previous Scores', 'Extracurricular Activities',\n       'Sleep Hours', 'Sample Question Papers Practiced', 'Performance Index'],\n      dtype='object')"},"metadata":{}}]},{"cell_type":"code","source":"data.columns = ['Hours_Studied', 'Previous_Scores', 'Extracurricular_Activities',\n       'Sleep_Hours', 'Sample_Question_Papers_ Practiced', 'Performance_Index']","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:57:03.403119Z","iopub.execute_input":"2024-06-21T18:57:03.403525Z","iopub.status.idle":"2024-06-21T18:57:03.409426Z","shell.execute_reply.started":"2024-06-21T18:57:03.403492Z","shell.execute_reply":"2024-06-21T18:57:03.408060Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"temp = [1 if data.iloc[i,2] ==\"Yes\" else 0 for i in range(len(data))]\ndata.Extracurricular_Activities = temp","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:57:04.423766Z","iopub.execute_input":"2024-06-21T18:57:04.424165Z","iopub.status.idle":"2024-06-21T18:57:04.706240Z","shell.execute_reply.started":"2024-06-21T18:57:04.424132Z","shell.execute_reply":"2024-06-21T18:57:04.705182Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"#### Baseline model","metadata":{}},{"cell_type":"code","source":"pred = data.Performance_Index.mean()\nmse = data.Performance_Index.var()\nprint(\"Baseline model MSE: \",mse)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:57:06.570705Z","iopub.execute_input":"2024-06-21T18:57:06.571136Z","iopub.status.idle":"2024-06-21T18:57:06.578726Z","shell.execute_reply.started":"2024-06-21T18:57:06.571105Z","shell.execute_reply":"2024-06-21T18:57:06.577336Z"},"trusted":true},"execution_count":7,"outputs":[{"name":"stdout","text":"Baseline model MSE:  369.1223771977198\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Linear Regression model   \n\n$y_{pred} = w.x + b$    \n\n*Loss function*  \n$L(w,b) = \\frac{1}{2m} {\\Sigma^m_{i=1}(y^i_{actual}-(w^i.x^i+b) )^2}$  \n\n*Derivative of loss function wrt to w*  \n$\\frac{df}{dw} L(w,b) = -\\frac{1}{m} {\\Sigma^m_{i=1}(y^i_{actual}-(w^i.x^i+b)).x^i}$  \n\n*Derivative of loss function wrt to b*  \n$\\frac{df}{db} L(w,b) = -\\frac{1}{m} {\\Sigma^m_{i=1}(y^i_{actual}-(w^i.x^i+b))}$  ","metadata":{}},{"cell_type":"markdown","source":"### Incorporating b in w \n\n$w_{twiddle} = (b,w)$   \n$w ∈ R^{1xd}$   \n$w_{twiddle} ∈ R^{1xd+1}$  \n\n$x_{twiddle} = (1,x)$   \n$x ∈ R^{rxd}$   \n$x_{twiddle} ∈ R^{rxd+1}$   \n\n$y_{pred} = w_{twiddle}.x_{twiddle}$  ","metadata":{}},{"cell_type":"markdown","source":"#### Data structure for w and x\n\n","metadata":{}},{"cell_type":"code","source":"# Input data and Output data\n\nx = np.array(data.iloc[:,0:5],ndmin=2)\ny = np.array(data.iloc[:,5],ndmin=1)\n\nprint(\"X shape:\",x.shape)\nprint(\"Y shape:\",y.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:57:09.543278Z","iopub.execute_input":"2024-06-21T18:57:09.543668Z","iopub.status.idle":"2024-06-21T18:57:09.557797Z","shell.execute_reply.started":"2024-06-21T18:57:09.543637Z","shell.execute_reply":"2024-06-21T18:57:09.556537Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"X shape: (10000, 5)\nY shape: (10000,)\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Predicting Y\n\n$y_{pred} = w.x + b$  \n$y_{pred} = w_{twiddle} . x_{twiddle}$","metadata":{}},{"cell_type":"code","source":"# Predicting y\n\ndef predict_y(x_twid,w_twid):\n    y_pred = np.dot(x_twid,w_twid)\n    return y_pred","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:57:11.124858Z","iopub.execute_input":"2024-06-21T18:57:11.125567Z","iopub.status.idle":"2024-06-21T18:57:11.131603Z","shell.execute_reply.started":"2024-06-21T18:57:11.125533Z","shell.execute_reply":"2024-06-21T18:57:11.130294Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"#### Derivative of Loss function\n\n$L(w) = \\frac{1}{2m} {\\Sigma^m_{i=1}(y^i_{actual}-w^i_{twiddle}.x^i_{twiddle} )^2}$  \n\n$\\frac{df}{dw} L(w) = -\\frac{1}{m} {\\Sigma^m_{i=1}(y^i_{actual}-w^i_{twiddle}.x^i_{twiddle}).x^i_{twiddle}}$  \n\n$ = -\\frac{1}{m} {\\Sigma^m_{i=1}(y^i_{actual}-y^i_{pred}).x^i_{twiddle}}$  ","metadata":{}},{"cell_type":"code","source":"# # Method 1 - Defining the loss function\n\n# def loss_fn_derivative(y_pred,y_actual,x_twiddle):\n    \n#     summation = -2 * np.mean((y_actual - y_pred)*x_twiddle)\n     \n#     #derivative should be of shape w_twiddle\n#     return summation","metadata":{"execution":{"iopub.status.busy":"2024-06-20T23:17:32.417965Z","iopub.execute_input":"2024-06-20T23:17:32.418475Z","iopub.status.idle":"2024-06-20T23:17:32.425928Z","shell.execute_reply.started":"2024-06-20T23:17:32.418436Z","shell.execute_reply":"2024-06-20T23:17:32.424451Z"},"trusted":true},"execution_count":47,"outputs":[]},{"cell_type":"code","source":"# Method 1 - Defining the loss function\n\n# def loss_fn_derivative(y_pred,y_actual,x_twiddle):\n#     r,d = x_twiddle.shape\n#     summation = np.zeros((r,d))\n#     for i in range(len(y_pred)):\n#         summation[i,] = ((y_actual[i] - y_pred[i])*x_twiddle[i,])\n    \n#     summation = -2 * np.mean(summation,axis = 0)\n\n#     #derivative should be of shape w_twiddle\n#     return summation","metadata":{"execution":{"iopub.status.busy":"2024-06-20T23:15:14.195368Z","iopub.execute_input":"2024-06-20T23:15:14.195784Z","iopub.status.idle":"2024-06-20T23:15:14.203814Z","shell.execute_reply.started":"2024-06-20T23:15:14.195752Z","shell.execute_reply":"2024-06-20T23:15:14.202277Z"},"trusted":true},"execution_count":43,"outputs":[]},{"cell_type":"code","source":"#Method 2 - Defining the loss function\n\ndef loss_fn_derivative(y_pred,y_actual,x_twiddle):\n    \n    y = y_actual - y_pred\n    summation = (y.reshape(-1,1)*x_twiddle)\n    summation = -2 * np.mean(summation,axis = 0)\n \n    #derivative should be of shape w_twiddle\n    return summation","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:57:29.934643Z","iopub.execute_input":"2024-06-21T18:57:29.935029Z","iopub.status.idle":"2024-06-21T18:57:29.940804Z","shell.execute_reply.started":"2024-06-21T18:57:29.935001Z","shell.execute_reply":"2024-06-21T18:57:29.939376Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"# Testing the loss derivative function\n\nxx = np.array([[1,2,3],[4,5,6],[7,8,9]])\nxt = np.array([[1,1,2,3],[1,4,5,6],[1,7,8,9]])\nprint(xx.shape)\nprint(xt.shape)\nww = np.array([1,2,3])\nb = 1\nyy_actual = np.array([14,32,50])\nwt = np.array([1,1,2,3])\nprint(ww.shape)\nprint(wt.shape)\n\nyy_pred = predict_y(xt,wt)\n\nprint(yy_actual[1],yy_actual.shape)\nprint(yy_pred[1],yy_pred.shape)\nprint(xt[1,:],yy_pred.shape)\n\nprint(loss_fn_derivative(yy_pred[1],yy_actual[1],xt[1,:]))\n#print(loss_fn_derivative(yy_pred,yy_actual,xt))","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:57:33.491009Z","iopub.execute_input":"2024-06-21T18:57:33.491406Z","iopub.status.idle":"2024-06-21T18:57:33.503384Z","shell.execute_reply.started":"2024-06-21T18:57:33.491369Z","shell.execute_reply":"2024-06-21T18:57:33.502191Z"},"trusted":true},"execution_count":12,"outputs":[{"name":"stdout","text":"(3, 3)\n(3, 4)\n(3,)\n(4,)\n32 (3,)\n33 (3,)\n[1 4 5 6] (3,)\n[ 2.  8. 10. 12.]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"#### Gradient descent to update w\n\n$w_{twiddle} = w_{twiddle} - \\alpha * \\frac{df}{dw} L(w)$  \n$\\alpha : Step Rate$","metadata":{}},{"cell_type":"code","source":"def gradient_descent(alpha,y_actual,y_pred,x_twid,w_twid):\n    derivative = loss_fn_derivative(y_pred,y_actual,x_twid)\n    w_twid = w_twid - (alpha * derivative)\n    return w_twid","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:57:36.632692Z","iopub.execute_input":"2024-06-21T18:57:36.633145Z","iopub.status.idle":"2024-06-21T18:57:36.639137Z","shell.execute_reply.started":"2024-06-21T18:57:36.633113Z","shell.execute_reply":"2024-06-21T18:57:36.637908Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"def get_training_data(x,y,n):\n    perm = np.random.permutation(len(x))\n    train_idx = perm[0:n]\n    test_idx = perm[n:]\n    train_x = x[train_idx,:]\n    train_y = y[train_idx]\n    test_x = x[test_idx,:]\n    test_y = y[test_idx]\n    return train_x, train_y, test_x, test_y","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:57:37.281713Z","iopub.execute_input":"2024-06-21T18:57:37.282126Z","iopub.status.idle":"2024-06-21T18:57:37.288222Z","shell.execute_reply.started":"2024-06-21T18:57:37.282093Z","shell.execute_reply":"2024-06-21T18:57:37.287128Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"train_x,train_y,test_x,test_y = get_training_data(x,y,9000)\n","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:57:38.194483Z","iopub.execute_input":"2024-06-21T18:57:38.194972Z","iopub.status.idle":"2024-06-21T18:57:38.202866Z","shell.execute_reply.started":"2024-06-21T18:57:38.194935Z","shell.execute_reply":"2024-06-21T18:57:38.201415Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"# Algorithm for linear egression using gradient research\n\n# x - input data\n# y - output data\n\ndef linear_reg_GD(train_x,train_y,n_iters=10):\n    \n    r,d = train_x.shape\n    ones = np.ones((len(train_x),1))\n    # Appending 1 in front of each row in X\n    x_twiddle = np.append(ones,train_x,axis=1) #X twiddle shape: (9000 , 6)\n    w = np.ones((d,))\n    b = np.zeros((1,)) \n    # Appending b to W array\n    w_twiddle = np.append(b,w,axis=0) #W twiddle shape: (6,)    \n    y_pred = predict_y(x_twiddle,w_twiddle)\n    \n    alpha = 0.0001\n    \n    for i in range(n_iters):\n        w_twiddle = gradient_descent(alpha,train_y,y_pred,x_twiddle,w_twiddle)\n        y_pred = predict_y(x_twiddle,w_twiddle)\n    \n    mse = mean_squared_error(train_y,y_pred)\n    print(\"Mse\", mse)\n       \n    return w_twiddle,y_pred","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:57:39.878205Z","iopub.execute_input":"2024-06-21T18:57:39.878567Z","iopub.status.idle":"2024-06-21T18:57:39.886004Z","shell.execute_reply.started":"2024-06-21T18:57:39.878541Z","shell.execute_reply":"2024-06-21T18:57:39.884807Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Algorithm for linear egression using stochastic gradient research\n\n# x - input data\n# y - output data\n\ndef linear_reg_stochasticGD(train_x,train_y,n_iters):\n    \n    r,d = train_x.shape\n    ones = np.ones((len(train_x),1))\n    # Appending 1 in front of each row in X\n    x_twiddle = np.append(ones,train_x,axis=1) #X twiddle shape: (9000 , 6)\n    w = np.ones((d,))\n    b = np.zeros((1,)) \n    # Appending b to W array\n    w_twiddle = np.append(b,w,axis=0) #W twiddle shape: (6,)    \n    y_pred = predict_y(x_twiddle,w_twiddle)\n    \n    loss_previous = 100000\n    alpha = 0.0001\n    \n    for i in range(n_iters):\n        loss_all_run = []\n        for i in range(len(train_y)):\n            w_twiddle = gradient_descent(alpha,train_y[i],y_pred[i],x_twiddle[i,:],w_twiddle)\n            y_pred[i] = predict_y(x_twiddle[i,:],w_twiddle)\n            loss = np.mean((y_pred[i]-train_y[i])**2)\n            loss_all_run.append(loss)\n            \n        loss_all_run = np.mean(loss_all_run)\n        if np.abs(loss_all_run - loss_previous) < 0.01:\n            print('Converged at run')\n            break\n        loss_previous = loss_all_run\n        \n    mse = mean_squared_error(train_y,y_pred)\n    print(\"Mse\", mse)\n       \n    return w_twiddle,y_pred","metadata":{"execution":{"iopub.status.busy":"2024-06-20T23:09:08.444552Z","iopub.execute_input":"2024-06-20T23:09:08.444956Z","iopub.status.idle":"2024-06-20T23:09:08.456941Z","shell.execute_reply.started":"2024-06-20T23:09:08.444922Z","shell.execute_reply":"2024-06-20T23:09:08.455675Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"t_bfre = time.time()\nw_twid, y_pred = linear_reg_GD(train_x,train_y,20000)\nt_aftr = time.time()","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:58:02.651409Z","iopub.execute_input":"2024-06-21T18:58:02.651797Z","iopub.status.idle":"2024-06-21T18:58:16.051021Z","shell.execute_reply.started":"2024-06-21T18:58:02.651741Z","shell.execute_reply":"2024-06-21T18:58:16.049842Z"},"trusted":true},"execution_count":17,"outputs":[{"name":"stdout","text":"Mse 28.194815694495095\n","output_type":"stream"}]},{"cell_type":"code","source":"print(t_aftr - t_bfre)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:58:18.777067Z","iopub.execute_input":"2024-06-21T18:58:18.777439Z","iopub.status.idle":"2024-06-21T18:58:18.784047Z","shell.execute_reply.started":"2024-06-21T18:58:18.777410Z","shell.execute_reply":"2024-06-21T18:58:18.782360Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"13.394461631774902\n","output_type":"stream"}]},{"cell_type":"code","source":"# Getting w and b from w twiddle\nprint(w_twid)\nw = w_twid[1:]\nb = w_twid[0]\nprint(f\"\\n y_pred = {w}.x + {b}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:58:20.695028Z","iopub.execute_input":"2024-06-21T18:58:20.695426Z","iopub.status.idle":"2024-06-21T18:58:20.701635Z","shell.execute_reply.started":"2024-06-21T18:58:20.695391Z","shell.execute_reply":"2024-06-21T18:58:20.700603Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"[-3.50845715  2.26070832  0.83555415 -0.35395114 -1.31647777 -0.2227843 ]\n\n y_pred = [ 2.26070832  0.83555415 -0.35395114 -1.31647777 -0.2227843 ].x + -3.5084571541667566\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Optimal solution: \n\n$w=(X^TX)^{−1} X^TY$","metadata":{}},{"cell_type":"code","source":"def test_optimal_solution(x,y,w):\n    x_T = np.transpose(x)\n    a = np.matmul(x_T,x)\n    b = np.matmul(x_T,y)\n    w_optimal = np.linalg.inv(x.T@x)@x.T@y\n    print(\"W optimal\",w_optimal)\n    print(\"W from GD\",w)\n    return w_optimal","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:58:26.454482Z","iopub.execute_input":"2024-06-21T18:58:26.454959Z","iopub.status.idle":"2024-06-21T18:58:26.461890Z","shell.execute_reply.started":"2024-06-21T18:58:26.454923Z","shell.execute_reply":"2024-06-21T18:58:26.460319Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"ones = np.ones((len(train_x),1))\nx_twid = np.append(ones,train_x,axis=1) #X twiddle shape: (9000 , 6)\n\nw_optimal = test_optimal_solution(x_twid,train_y,w_twid)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:58:30.067219Z","iopub.execute_input":"2024-06-21T18:58:30.067591Z","iopub.status.idle":"2024-06-21T18:58:30.087951Z","shell.execute_reply.started":"2024-06-21T18:58:30.067563Z","shell.execute_reply":"2024-06-21T18:58:30.086213Z"},"trusted":true},"execution_count":21,"outputs":[{"name":"stdout","text":"W optimal [-33.99996861   2.8527092    1.01784049   0.59507843   0.47638189\n   0.19549017]\nW from GD [-3.50845715  2.26070832  0.83555415 -0.35395114 -1.31647777 -0.2227843 ]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"w =  [2.85178776 1.01803091 0.62725788 0.4816724  0.19402352]   \nb =  -34.05322184418339","metadata":{}},{"cell_type":"code","source":"y_optimal = predict_y(x_twid,w_optimal)","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:58:44.774105Z","iopub.execute_input":"2024-06-21T18:58:44.775076Z","iopub.status.idle":"2024-06-21T18:58:44.781435Z","shell.execute_reply.started":"2024-06-21T18:58:44.775023Z","shell.execute_reply":"2024-06-21T18:58:44.780159Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"print(y_optimal[0:10])\nprint(train_y[0:10])\nprint(y_pred[0:10])","metadata":{"execution":{"iopub.status.busy":"2024-06-21T18:58:47.297149Z","iopub.execute_input":"2024-06-21T18:58:47.298291Z","iopub.status.idle":"2024-06-21T18:58:47.305599Z","shell.execute_reply.started":"2024-06-21T18:58:47.298249Z","shell.execute_reply":"2024-06-21T18:58:47.304139Z"},"trusted":true},"execution_count":23,"outputs":[{"name":"stdout","text":"[35.65885759 49.90030249 23.0184601  44.13512948 33.68647706 48.8534164\n 51.05687839 91.47337624 66.54768507 47.10092715]\n[35. 45. 27. 47. 34. 47. 52. 93. 69. 49.]\n[44.76637614 49.15794809 32.72878398 41.63676021 35.31238903 52.54899409\n 53.60839446 85.74226505 66.30784611 51.38304116]\n","output_type":"stream"}]}]}